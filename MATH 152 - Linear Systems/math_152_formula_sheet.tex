\documentclass[10pt]{article}
% Packages
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{multicol}
\usepackage[landscape, total={10.8in,8in}]{geometry}
\usepackage{blindtext}
\usepackage{graphicx}
\usepackage{tikz}
\usepackage{enumitem}
\usepackage{array}
% Page formatting
\pagenumbering{gobble}
\parindent=0pt
% Symbols
\newcommand{\C}{{\mathbb C}}
\newcommand{\N}{{\mathbb N}}
\newcommand{\R}{{\mathbb R}}
\newcommand{\Z}{{\mathbb Z}}
\newcommand{\ep}{\varepsilon}
\newcommand{\ihat}{\hat{i}}
\newcommand{\jhat}{\hat{j}}
\newcommand{\khat}{\hat{k}}
% Vector commands
\newcommand{\proj}{\mathrm{proj}}
\newcommand{\rank}{\mathrm{rank}}
\newcommand\norm[1]{\lVert#1\rVert}
\newcommand\vect[1]{\overrightarrow{#1}}
% Matrix commands
\newenvironment{amatrix}[1]{\left[\begin{array}{@{}*{#1}{c}|c@{}}}{\end{array}\right]}
\newcommand{\matr}[1]{\mathbf{#1}}
\newcommand{\tran}{^{\mkern-1.5mu\mathsf{T}}}
\newcommand{\conj}[1]{\overline{#1}}
\newcommand{\inv}{^{-1}}
% Complex number commands
\newcommand{\cis}{\mathrm{cis }}
% Heading
\newcommand\sectionheading[1]{\begin{center}\large{\textbf{#1}}\end{center}\normalsize}
\newcommand\heading[1]{\medskip\textbf{#1}\medskip}

\begin{document}

\begin{center}
    \huge{\textbf{MATH 152 Formula Sheet}}
\end{center}

\begin{multicols*}{3}

\sectionheading{Vectors}

\heading{Basics}

\begin{tabular}{@{}ll}
    Direction Vector & $\vect{ab}=\vec b-\vec a$ \\
    Norm & $\norm{\vec a} =\sqrt{a_1^2+\cdots+a_n^2}$ \\
    Unit Vector & $\hat{u}=\frac{\vec u}{\norm{u}}$ 
\end{tabular}

\heading{Dot Product}

$\vec a \cdot \vec b = a_1 b_1 + a_2 b_2 + \cdots + a_n b_n$

$\vec a\cdot\vec b=\vec b\cdot\vec a$

$\vec a \cdot \vec b = \norm{\vec a}\norm{\vec b}\cos\theta$

$\vec a \perp \vec b$ if $\vec a \cdot\vec b = 0$ 

\heading{Projection and Perpendicular}

$\proj_{\vec b}(\vec a)=\frac{\vec a \cdot \vec b}{\vec b\cdot\vec b}\vec b=\frac{\vec a \cdot \vec b}{\norm{\vec b}^2}\vec b=(\vec a\cdot \hat{b})\hat{b}$

$\text{perp}_{\vec b}(\vec a)=\vec a-\proj_{\vec b}(\vec a)$

\heading{Cross Product}

$\vec a\times\vec b=\det\begin{bmatrix}
    \ihat & \jhat & \khat \\
    a_1 & a_2 & a_3 \\
    b_1 & b_2 & b_3
\end{bmatrix}$

$\vec a\times\vec b=-\vec b\times\vec a$

$\norm{\vec a\times\vec b}=\norm{\vec a}\norm{\vec b}\sin\theta$

$\vec a\parallel\vec b$ if $\vec a\times\vec b=\vec 0$.

\heading{Area and Volume}

$A=\norm{\vec a\times\vec b}$

$A=\left|\det\begin{bmatrix}
    -\vec a- \\
    -\vec b- 
\end{bmatrix}\right|$

$V=|\vec a\cdot(\vec b\times\vec c)|$

$V=\left|\det\begin{bmatrix}
    -\vec a- \\
    -\vec b- \\
    -\vec c- 
\end{bmatrix}\right|$

\sectionheading{Lines and Planes}

\heading{Line Equations}

\begin{tabular}{@{}ll}
    Vector/Parametric & $\vec x = \vec p + \vec a t$ \\
    Two-Point & $\vec x = (1-t)\vec a + t\vec b$ \\
    Point-Normal Form in $\R^2$ & $\vec n \cdot (\vec x - \vec p) = 0$ \\
    Standard Form in $\R^2$ & $ax+by=c$ where \\
    & $\vec n=\langle a,b\rangle$ and $c=\vec n\cdot\vec p$
\end{tabular}

\heading{Plane Equations in $\R^3$}

\begin{tabular}{@{}ll}
    Vector/Parametric & $\vec x=\vec p+\vec a s + \vec b t$ \\
    Point-Normal Form & $\vec n \cdot (\vec x - \vec p) = 0$ \\
    Standard Form in $\R^3$ & $ax+by+cz=d$ where\\
    & $\vec n=\langle a,b,c\rangle$ and $d=\vec n\cdot \vec p$ \\
    Two Lines/Three Points & Use $\vec a$, $\vec b$ and $\vec n=\vec a\times\vec b$ \\
\end{tabular}

\heading{Hyperplanes}

A hyperplane has dimension $n-1$ in $\R^n$

\begin{tabular}{@{}ll}
    Point-Normal Form & $\vec n \cdot (\vec x - \vec p) = 0$ \\
    Standard Form & $a_1x_1+\cdots+a_nx_n=d$ where \\
    & $\vec n=\langle a_1,\ldots a_n\rangle$ and $\vec d=\vec n\cdot\vec p$
\end{tabular}

\heading{Distance Between Objects}

Distance between point $\vec q$ and line $\vec x=\vec p+\vec a t$:

$d=\norm{\mathrm{perp}_{\vec a}(\vect{pq})}=\norm{\proj_{\vec a^\perp}(\vect{pq})}$

Distance between point $\vec q$ and hyperplane with point $\vec p$:

$d=\norm{\proj_{\vec n}(\vect{pq})}$

\heading{Intersection of Objects}

Use parametric forms and see if solutions for parameters are consistent.

\sectionheading{Linear Systems}

\heading{Gauss-Jordan Elimination}

\begin{enumerate}[noitemsep,topsep=0pt]
    \item Set the top left entry to $1$
    \item Use the first row to `kill off' other entries in the first column
    \item For column $2$, use one row to `kill off' other entries in that column
    \item Repeat process until the matrix is in RREF
\end{enumerate}

\heading{Solutions to Linear Systems}

$\rank$: number of leading $1$s in the RREF. $n$: number of unknowns.

\begin{itemize}[noitemsep,topsep=0pt]
    \item If $\rank(\matr{A})<\rank([\matr{A}\mid\vec b])$, the system is inconsistent.
    \item If $\rank(\matr{A})=\rank([\matr{A}\mid\vec b])=n$, there is a unique solution.
    \item If $\rank(\matr{A})=\rank([\matr{A}\mid\vec b])<n$, there are infinitely many solutions. $k$-parameter family of solutions where $k=n-\rank(A)$.
\end{itemize}

\heading{Polynomial Interpolation}

With points $(x_1,y_1),\ldots,(x_n,y_n)$ and $p(x)=r_0+r_1x+\dots+r_{n-1}x^{n-1}$, solve:

\[\begin{amatrix}{4}
    1 & x_1 & \cdots & x_1^{n-1} & y_1 \\
    \vdots & \vdots & \ddots & \vdots & \vdots \\
    1 & x_n & \cdots & x_n^{n-1} & y_n
\end{amatrix}\]

\sectionheading{Matrices}

\heading{Matrix Multiplication}

$\matr{A}_{m\times p}\matr{B}_{p\times n}=(\matr{A}\matr{B})_{m\times n}$

$\matr{A}\matr{I}=\matr{I}\matr{A}=\matr{A}$

$\begin{bmatrix}
    -\vec a_1- \\
    % -\vec a_2- \\
    \vdots \\
    -\vec a_m- 
\end{bmatrix} \begin{bmatrix}
    \vert & & \vert \\
    \vec b_1 & \cdots & \vec b_n \\
    \vert & & \vert 
\end{bmatrix}=\begin{bmatrix}
    \vec a_1 \cdot \vec b_1 & \cdots & \vec a_1 \cdot \vec b_n \\
    \vdots & \ddots & \vdots  \\
    \vec a_m \cdot \vec b_1 & \cdots & \vec a_n \cdot \vec b_n
\end{bmatrix}$

\heading{Transpose}

$(\matr{A}\tran)\tran=\matr{A}$

$(\matr{A}+\matr{B})\tran=\matr{A}\tran+\matr{B}\tran$

$(\matr{A}\matr{B})\tran=\matr{B}\tran\matr{A}\tran$

$(k\matr{A})\tran=k\matr{A}\tran$

Symmetric: $\matr{A}\tran=\matr{A}$

Skew-symmetric: $\matr{A}\tran=-\matr{A}$

\heading{Inverse}

$\matr{A}$ is invertible if $\det(\matr{A})\ne0$.

$\matr{A}\inv\matr{A}=\matr{A}\matr{A}\inv=\matr{I}$

$(\matr{A}\inv)\inv=\matr{A}$

$(k\matr{A})\inv=\frac 1k\matr{A}\inv$

$(\matr{A}\tran)\inv=(\matr{A}\inv)\tran$

$(\matr{A}\matr{B})\inv=\matr{B}\inv\matr{A}\inv$

$2\times2$ matrix inverse:

$\begin{bmatrix}
    a & b \\
    c & d
\end{bmatrix}\inv = \dfrac{1}{ad-bc}\begin{bmatrix}
    d & -b \\
    -c & a
\end{bmatrix}$

General matrix inverse:

\begin{enumerate}[noitemsep,topsep=0pt]
    \item Augment $[\matr{A}\mid\matr{I}]$  
    \item Use Gauss-Jordan Elimination to row reduce $\matr{A}$ to $\matr{I}$, creating the matrix $[\matr{I}\mid\matr{X}]$
    \item $\matr{A}\inv=\matr{X}$.
\end{enumerate}

\heading{Elementary Matrices}

$(\matr{E}_k\cdots\matr{E}_3\matr{E}_2\matr{E}_1)\matr{A}=\matr{I}$

$\matr{A}=\matr{E}_1\inv\matr{E}_2\inv\cdots\matr{E}_k\inv$

\heading{Invertible Matrix Theorem}

\begin{itemize}[noitemsep,topsep=0pt]
    \item $\matr{A}$ is invertible 
    \item $\matr{A}\tran$ is invertible 
    \item $\rank(\matr{A})=n$
    \item The RREF of $\matr{A}_{n\times n}$ is $\matr{I}_n$
    \item $\matr{A}$ is a product of elementary matrices
    \item The linear system $\matr{A}\vec x=\vec b$ has a unique solution 
    \item The homogeneous system $\matr{A}\vec x=\vec 0$ has only the trivial solution 
    \item $\det(A)\ne 0$
    \item $0$ is not an eigenvalue of $\matr{A}$
\end{itemize}

\heading{Matrix Equations}

$\matr{A}\vec x=\vec b \implies \vec x = \matr{A}\inv\vec b$

\heading{Linear Transformations}

$T(\vec x)=\matr{A}\vec x$

$T(\vec x+\vec y)=T(\vec x)+T(\vec y)$

$T(s\vec x)=sT(\vec x)$

$\matr{A}=[T(\vec e_1)\mid T(\vec e_2)\mid\cdots\mid T(\vec e_n)]$

$(S\circ T)(\vec x)=S(T(\vec x))=\matr{B}\matr{A}\vec x$

Inverse of linear transformation: $(S\circ T)(\vec x)=\vec x$. The inverse transformation $T\inv$ is induced by the matrix $\matr{A}\inv$.

\heading{Rotations}

$\mathrm{Rot}_{\theta}=\begin{bmatrix}
    \cos\theta & -\sin\theta \\
    \sin\theta & \cos\theta 
\end{bmatrix}$

\heading{Reflections}

$\mathrm{Ref}_{m}=\dfrac{1}{1+m^2}\begin{bmatrix}
    1 - m^2 & 2m \\
    2m & m^2 - 1
\end{bmatrix}$

$\mathrm{Ref}_{\theta}=\begin{bmatrix}
    \cos(2\theta) & \sin(2\theta) \\
    \sin(2\theta) & -\cos(2\theta)
\end{bmatrix}$

\heading{Compositions}

$\mathrm{Rot}_{\theta}\circ\mathrm{Rot}_{\phi}=\mathrm{Rot}_{\theta+\phi}$

$\mathrm{Ref}_{\theta}\circ\mathrm{Ref}_{\phi}=\mathrm{Rot}_{2(\theta-\phi)}$

$\mathrm{Rot}_{\theta}\circ\mathrm{Ref}_{\phi}=\mathrm{Ref}_{\phi+\theta/2}$

$\mathrm{Ref}_{\theta}\circ\mathrm{Rot}_{\phi}=\mathrm{Ref}_{\phi-\theta/2}$

\heading{Projections}

$\mathrm{Proj}_{m}=\dfrac{1}{1+m^2}\begin{bmatrix}
    1 & m \\
    m & m^2
\end{bmatrix}$

$\mathrm{Proj}_{\theta}=\dfrac{1}{2}\begin{bmatrix}
    \cos^2\theta & \cos\theta\sin\theta \\
    \cos\theta\sin\theta & \sin^2 \theta
\end{bmatrix}$

$\mathrm{Proj}_{\theta}=\dfrac{1}{2}\begin{bmatrix}
    1+\cos 2\theta & \sin 2\theta \\
    \sin 2\theta & 1 - \cos 2\theta  
\end{bmatrix}$

\heading{Simple Determinants}

$\det([a])=a$

$\det\begin{bmatrix}
    a & b \\
    c & d
\end{bmatrix} = \begin{vmatrix}
    a & b \\
    c & d
\end{vmatrix} = ad-bc$

$\det\begin{bmatrix}
    a & b & c \\
    d & e & f \\
    g & h & i
\end{bmatrix} = a\begin{vmatrix}
    e & f \\ 
    h & i
\end{vmatrix} - b\begin{vmatrix}
    d & f \\
    g & i
\end{vmatrix} + c\begin{vmatrix}
    d & e \\
    g & h
\end{vmatrix}$

\heading{Determinant Properties}

If $\matr{A}$ has a zero row or zero column, then $\det(\matr{A})=0$

If two rows or two columns are scalar multiples, then $\det(\matr{A})=0$

$\det(\matr{A})=\det(\matr{A}\tran)$

$\det(\matr{A}\inv)=\frac{1}{\det(\matr{A})}$

$\det(\matr{A}^x)=\det(\matr{A})^x$

$\det(\matr{A}\matr{B})=\det(\matr{A})\det(\matr{B})$

$\det(k\matr{A})=k^n\det(\matr{A})$

If $\matr{A}$ is triangular, then $\det(\matr{A})$ is equal to the product of the entires on the main diagonal.

\heading{Simplifying Determinants}

\begin{tabular}{@{}ll}
    Swap Rows & $\det(\matr{B})=-\det(\matr{A})$ \\
    Multiply Row by $k$ & $\det(\matr{B})=k\det(\matr{A})$ \\
    Add Factor of a Row & $\det(\matr{B})=\det(\matr{A})$
\end{tabular}

These rules also apply if operations are performed on columns.

\heading{Calculating Determinants}

Minor: $M_{ij}$ is the determinant of the submatrix that remains after removing row $i$ and column $j$

Cofactor: $C_{ij}=(-1)^{i+j}M_{ij}$

$\det(\matr{A})$ is equal to the products of entries and cofactors along any row or column

\heading{Determinants and Inverse}

$\matr{C_A}$: matrix of cofactors.

Adjoint matrix: $\mathrm{adj}(\matr{A})=(\matr{C_A})\tran$

$\matr{A}\inv=\frac{1}{\det(A)}\mathrm{adj}(\matr{A})$

\heading{Cramer's Rule}

Let $\matr{A}_{n\times n}$ be an invertible matrix, and let $\vec b\in\R^n$ be a constant vector.

Define $\matr{A}_i$ to be matrix $\matr{A}$ with column $i$ replaced by $\vec b$. 

Then, $x_i=\frac{\det(\matr{A}_i)}{\det(\matr{A})}$

\sectionheading{Complex Numbers}

\heading{Definitions}

\begin{tabular}{@{}ll}
    Imaginary Number & $i^2=-1$ \\
    Complex Number & $z=a+bi$ \\
    Conjugate & $\conj{z}=a-bi$ \\
    Real Part & $\Re(z)=\frac{z+\conj{z}}{2}$ \\
    Imaginary Part & $\Im{z}=\frac{z-\conj{z}}{2}$ \\
    Norm & $|z|=\sqrt{a^2+b^2}$
\end{tabular}

\heading{Operations and Identities}

$\conj{(z\pm u)}=\conj{z}\pm\conj{u}$

$\conj{zu}=\conj{z}\cdot\conj{u}$

$\conj{\frac{z}{u}}=\frac{\conj{z}}{\conj{u}}$

$z\cdot\conj{z}=|z|^2$

$|zu|=|z||u|$

$\frac{u}{z}=\frac{u\conj{z}}{|z|^2}$

\heading{Polar Form}

$z=r(\cos\theta+i\sin\theta)=re^{i\theta}$

$\arg(z)=\theta=\arctan(\frac{b}{a})$

$z\cdot w=(re^{i\theta})\cdot(se^{i\phi})=(rs)e^{i(\theta+\phi)}$

\heading{Powers and Roots}

$z^n=r^n(\cos(n\theta)+i\sin(n\theta))$

Solving $z^n=w$, where $w=se^{i\phi}$:

$\begin{array}{ccl}
    z_1 & = & \sqrt[n]{s}e^{i(\phi)/n} \\
    z_2 & = & \sqrt[n]{s}e^{i(\phi+2\pi\cdot 1)/n} \\
    & \vdots & \\
    z_n & = & \sqrt[n]{s}e^{i(\phi+2\pi\cdot(n-1))/n} 
\end{array}$

\sectionheading{Vector Spaces}

\heading{Vector Space Axioms}

\begin{enumerate}[noitemsep,topsep=0pt]
    \item $\vec u+\vec v$ must be in $V$ (Closure)
    \item $k\cdot\vec v$ must be in $V$ (Closure)
    \item $(\vec u+\vec v)+\vec w=\vec u+(\vec v+\vec w)$ (Associativity)
    \item $k\cdot(m\cdot\vec v)=(km)\cdot\vec v$ (Associativity)
    \item $k\cdot(\vec u+\vec v)=k\cdot\vec u+k\cdot\vec v$ (Distributivity)
    \item $(k+m)\cdot\vec v=k\cdot\vec v+m\cdot\vec v$ (Distributivity)
    \item $\vec u+\vec v=\vec v+\vec u$ (Commutativity)
    \item $\vec v+\vec 0=\vec 0+\vec v=\vec v$ (Additive Identity)
    \item $\vec v+(-\vec v)=\vec 0$ (Additive Inverse)
    \item $1\cdot \vec v=\vec v$ (Multiplicative Identity)
\end{enumerate}

\heading{Vector Subspace}

\begin{enumerate}[noitemsep,topsep=0pt]
    \item $W$ contains the zero vector of $V$: $\vec 0\in W$
    \item $W$ is closed under vector addition: $\vec w_1+\vec w_2\in W$
    \item $W$ is closed under scalar multiplication: $k\cdot\vec w_1\in W$
\end{enumerate}

It suffices to show that $a\vec w_1+b\vec w_2\in W$

\heading{Linear Independence}

$\vec v_1,\vec v_2,\ldots,\vec v_n$ are linearly independent

If $\begin{amatrix}{4}
    \vert & \vert & & \vert & \vert \\
    \vec v_1 & \vec v_2 & \cdots & \vec v_n & \vec 0 \\
    \vert & \vert & & \vert & \vert
\end{amatrix}$ has only the trivial solution

Or if $\det\begin{bmatrix}
    \vert & \vert & & \vert \\
    \vec v_1 & \vec v_2 & \cdots & \vec v_n \\
    \vert & \vert & & \vert
\end{bmatrix}\ne 0$

\heading{Span}

If $S=\{\vec v_1, \vec v_2, \ldots, \vec v_n\}$, then $\mathrm{span}(S)$ is the set of all linear combinations of the vectors in $S$.

Checking if $\vec b\in\mathrm{span}(S)$: solve 

$\begin{amatrix}{4}
    \vert & \vert & & \vert & \vert \\
    \vec v_1 & \vec v_2 & \cdots & \vec v_n & \vec b \\
    \vert & \vert & & \vert & \vert 
\end{amatrix}$

If $V$ is a vector space and $\mathrm{span}(S)=V$, then $S$ is a generating set of $V$

\heading{Basis}

If $B$ is linearly independent and a generating set of $V$, then $B$ is a basis for $V$

\heading{Dimension}

The dimension of a vector space $V$ is the number of vectors in its basis, denoted as $\dim(V)$

\heading{Coordinates}

If $B=\{\vec v_1, \vec v_2, \ldots, \vec v_n\}$ is a basis of $V$, and $\vec v=c_1\vec v_1 + c_2\vec v_2 + \cdots + c_n\vec v_n$, then 

$\begin{bmatrix}
    c_1 \\
    c_2 \\
    \vdots \\
    c_n
\end{bmatrix}$ are the coordinates of $\vec v$ relative to the basis $B$.

\heading{Column and Null Space}

Given a matrix $\matr{A}_{n\times n}$:

Column space: $\mathrm{Col}(\matr{A})=\{\vec b\in\R^n\mid\matr{A}\vec x=\vec b\}$ where $\vec x\in\R^n$

Column space: $\mathrm{Col}(\matr{A})=\mathrm{span}(\{\vec a_1,\ldots,\vec a_n\})$ (column vectors)

Null space: $\mathrm{Null}(\matr{A})=\{\vec x\in\R^n\mid\matr{A}\vec x=\vec 0\}$

$\rank(\matr{A})+\dim(\mathrm{Null}(\matr{A}))=n$

\sectionheading{Eigen-Analysis}

\heading{Eigenvectors and Eigenvalues}

Consider $\matr{A}_{n\times n}$. $\vec x\in\R^n$ is an eigenvector of $\matr{A}$ with associated eigenvalue $\lambda\in\R$ if:

$\matr{A}\vec x=\lambda\vec x$

\heading{Characteristic Polynomial}

$p_A(\lambda)=\det(\matr{A}-\lambda\matr{I})$

Eigenvalues: $\lambda$ such that $p_A(\lambda)=0$

\heading{Solving Method}

\begin{enumerate}[noitemsep,topsep=0pt]
    \item Use $\det(\matr{A}-\lambda\matr{I})=0$ to solve for $\lambda$
    \item Use $(\matr{A}-\lambda_i\matr{I})\vec x_i=\vec 0$ to solve for $\vec x_i$. Each eigenvector is not unique, scalar multiples are valid
\end{enumerate}

Algebraic multiplicity: number of times the eigenvalue appears as a root 

Geometric multiplicty: number of corresponding eigenvectors for the eigenvalue

$2\times 2$ matrices:

$\matr{A}-\lambda\matr{I}=\begin{bmatrix}
    a & b \\
    ? & ?
\end{bmatrix}\implies\vec x=\begin{bmatrix}
    -b \\
    a
\end{bmatrix}$

\heading{Properties}

\begin{tabular}{@{}ll}
    Triangular Matrix & Diagonal entries are eigenvalues \\
\end{tabular}

$\matr{A}^m\vec x=\lambda^m\vec x$

$\det(\matr{A})=\lambda_1\times\lambda_2\times\cdots\times\lambda_n$

$\mathrm{tr}(\matr{A})=a_11+a_22+\cdots+a_nn=\lambda_1+\lambda_2+\cdots+\lambda_n$

If $\matr{P}$ is an invertible matrix, then $\matr{A}$ is similar to $\matr{P}\inv\matr{A}\matr{P}$ and have the same eigenvalues

\heading{Diagonalization}

$\matr{A}$ is diagonalizable if $\matr{A}$ is similar to a diagonal matrix $\matr{D}$, where $\matr{D}=\matr{P}\inv\matr{A}\matr{P}$

$\matr{P}=\begin{bmatrix}
    \vert & \vert & & \vert \\
    \vec x_1 & \vec x_2 & \cdots & \vec x_n \\
    \vert & \vert & & \vert 
\end{bmatrix}$ (matrix of eigenvectors)

$\matr{D}=\begin{bmatrix}
    \lambda_1 & 0 & \cdots & 0 \\
    0 & \lambda_2 & \cdots & 0 \\
    \vdots & \vdots & \ddots & 0 \\
    0 & 0 & 0 & \lambda_n
\end{bmatrix}$ (eigenvalues in same order)

Diagonalization Theorem 

\begin{enumerate}[noitemsep,topsep=0pt]
    \item $\matr{A}_{n\times n}$ is diagonalizable if and only if $\matr{A}$ has $n$ linearly independent eigenvectors 
    \item Or if algebratic multiplicty matches geometric multiplicity for all eigenvalues 
    \item Or if $\matr{A}$ has $n$ distinct eigenvalues
\end{enumerate}

$\matr{A}^k=\matr{P}\matr{D}^k\matr{P}\inv$. For diagonal matrices,

$\matr{D}^k=\begin{bmatrix}
    d_1^k & 0 & \cdots & 0 \\
    0 & d_2^k & \cdots & 0 \\
    \vdots & \vdots & \ddots & 0 \\
    0 & 0 & 0 & d_n^k
\end{bmatrix}$

\heading{Markov Chains}

A matrix whose columns are numbers between $[0,1]$, which sum to $1$ 

$a_{ij}$: probability of moving from $j$ to $i$

State vector: $\vec x_n=\begin{bmatrix}
    x_{1n} \\
    \vdots \\
    x_{nn}
\end{bmatrix}$ (each amount at time $n$)

Markov Chain equation: $\vec x_{n+1}=\matr{A}\vec x_n$

Steady state vector: $\vec x_s=\matr{A}\vec x_s$. 

$\vec x_n$ approaches $\vec x_s$ for large $n$

$(\matr{I}-\matr{A})\vec x_s=\vec 0$, solve using Gauss-Jordan elimination 

\heading{Linear Dynamical Systems}

$\vec v_{n+1}=\matr{A}\vec v_n$

$\vec v_n=\matr{A}^n\vec v_0=\matr{P}\matr{D}^n\matr{P}\inv\vec v_0$

\heading{Recurrence Relations}

$x_{n+2}=cx_n+dx_{n+1}$, $x_0=a$, $x_1=b$. Let $y_n=x_{n+1}$

$\begin{bmatrix}
    x_{n+1} \\
    y_{n+1}
\end{bmatrix} = \begin{bmatrix}
    0 & 1 \\
    c & d
\end{bmatrix} \begin{bmatrix}
    x_n \\
    y_n
\end{bmatrix}$

\sectionheading{Differential Equations}

\heading{Simple Differential Equation}

Solution to $y'=ay$ is $y(x)=ce^{ax}$

\heading{System of Linear Differential Equations}

$\begin{bmatrix}
    y_1' \\
    y_2' \\
    \vdots \\
    y_n'
\end{bmatrix}=\begin{bmatrix}
    a_{11} & a_{12} & \cdots & a_{1n} \\
    a_{21} & a_{22} & \cdots & a_{2n} \\
    \vdots & \vdots & \ddots & \vdots \\
    a_{n1} & a_{n2} & \cdots & a_{nn}
\end{bmatrix}\begin{bmatrix}
    y_1 \\
    y_2 \\
    \vdots \\
    y_n
\end{bmatrix}$

\begin{enumerate}[noitemsep,topsep=0pt]
    \item Set $\vec y'=\matr{A}y$
    \item Find the eigenvalues and eigenvectors of $\matr{A}$
    \item $\vec x(t)=(c_1e^{\lambda_1 x})\vec x_1+(c_2e^{\lambda_2 x})\vec x_2+\cdots+(c_ne^{\lambda_n x})\vec x_n$
\end{enumerate}

\heading{Higher Order Differential Equation}

Substitute $y_1=y,y_2=y',\ldots$

Convert to system of linear differential equations with single derivatives on the left hand side

\end{multicols*}

\end{document}